{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45aa6001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from bidi.algorithm import get_display\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae62d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = list(codecs.open(filename, 'r', 'utf-8').readlines())\n",
    "    x, y = zip(*[d.strip().split('\\t') for d in data])\n",
    "    # Reducing any char-acter sequence of more than 3 consecutive repetitions to a respective 3-character sequence \n",
    "    # (e.g. “!!!!!!!!”turns to “!!!”)\n",
    "    # x = [re.sub(r'((.)\\2{3,})', r'\\2\\2\\2', i) for i in x]\n",
    "    x = np.asarray(list(x))\n",
    "    y = np.array(y).astype(np.int)\n",
    "    #y = to_categorical(y, 3)\n",
    "    \n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd558a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae5d4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7f76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "  return ''.join(c for c in text if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", string)\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5383472",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8336372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data, ngrams=(1, 1)):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngrams)\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c749d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e4988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "140ad809",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_token_train, y_token_train = load_data('data/token_train.tsv')\n",
    "x_token_test, y_token_test = load_data('data/token_test.tsv')\n",
    "x_morph_train, y_morph_train = load_data('data/morph_train.tsv')\n",
    "x_morph_test, y_morph_test = load_data('data/morph_test.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61be1e88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\" שמע ישראל , ה שם ישמור ו יקרא ה גורל =  ( י.ק.ו.ק . ) אימרו אמן ל אבא ה שם של אנחנו ! ! ! ! אחרי ברכה של ביבי ! ה כח ב ה ישראל הוא מתי ש יש משמעת ו פרגמתיות ב משרדי ה חינוך ש זה איתן את ה אור ! ש מאוד חסר ל אנחנו ! , ו התאחדות ב אחד שלם , ו אין שמאל ו אין ימין ! ו ב ישראל נקודה חשובה היא , תעשיית כוח פרגמטיבית ! https://www.youtube.com/watch?v=_rKMXgPQSj8 . עוד מעת אהיה ראש חודש תעברו על ה תפילה של ה תיקון ה כללי ו תדליקו את ה נר ! \"',\n",
       "       'איחולי הצלחה ב תפקידך .', 'כל ה כבוד !!!', ...,\n",
       "       'מה קרה נהיית נשיא נהיית פרס ? ל אתה תתעסק ב הסתות של ה ערבים ו אחר כך תבוא ב טענות טמבל',\n",
       "       'כבוד ה נשיא ה נבחר . התרגשתי אתמול מ נאומך ב עצרת ה הזדהות ב כיכר רבין . אתה ראוי לכבוד ל הוא זכית !',\n",
       "       'ה דמעות זולגות מ עצמן'], dtype='<U2436')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_morph_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ef666d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8f8ddba0d92a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_morph_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "x_morph_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d080be2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f1223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54e8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hebrew_stop_words = ['אני',\n",
    "'את',\n",
    "'אתה',\n",
    "'אנחנו',\n",
    "'אתן',\n",
    "'אתם',\n",
    "'הם',\n",
    "'הן',\n",
    "'היא',\n",
    "'הוא',\n",
    "'שלי',\n",
    "'שלו',\n",
    "'שלך',\n",
    "'שלה',\n",
    "'שלנו',\n",
    "'שלכם',\n",
    "'שלכן',\n",
    "'שלהם',\n",
    "'שלהן',\n",
    "'לי',\n",
    "'לו',\n",
    "'לה',\n",
    "'לנו',\n",
    "'לכם',\n",
    "'לכן',\n",
    "'להם',\n",
    "'להן',\n",
    "'אותה',\n",
    "'אותו',\n",
    "'זה',\n",
    "'זאת',\n",
    "'אלה',\n",
    "'אלו',\n",
    "'תחת',\n",
    "'מתחת',\n",
    "'מעל',\n",
    "'בין',\n",
    "'עם',\n",
    "'עד',\n",
    "'נגר',\n",
    "'על',\n",
    "'אל',\n",
    "'מול',\n",
    "'של',\n",
    "'אצל',\n",
    "'כמו',\n",
    "'אחר',\n",
    "'אותו',\n",
    "'בלי',\n",
    "'לפני',\n",
    "'אחרי',\n",
    "'מאחורי',\n",
    "'עלי',\n",
    "'עליו',\n",
    "'עליה',\n",
    "'עליך',\n",
    "'עלינו',\n",
    "'עליכם',\n",
    "'לעיכן',\n",
    "'עליהם',\n",
    "'עליהן',\n",
    "'כל',\n",
    "'כולם',\n",
    "'כולן',\n",
    "'כך',\n",
    "'ככה',\n",
    "'כזה',\n",
    "'זה',\n",
    "'זות',\n",
    "'אותי',\n",
    "'אותה',\n",
    "'אותם',\n",
    "'אותך',\n",
    "'אותו',\n",
    "'אותן',\n",
    "'אותנו',\n",
    "'ואת',\n",
    "'את',\n",
    "'אתכם',\n",
    "'אתכן',\n",
    "'איתי',\n",
    "'איתו',\n",
    "'איתך',\n",
    "'איתה',\n",
    "'איתם',\n",
    "'איתן',\n",
    "'איתנו',\n",
    "'איתכם',\n",
    "'איתכן',\n",
    "'יהיה',\n",
    "'תהיה',\n",
    "'היתי',\n",
    "'היתה',\n",
    "'היה',\n",
    "'להיות',\n",
    "'עצמי',\n",
    "'עצמו',\n",
    "'עצמה',\n",
    "'עצמם',\n",
    "'עצמן',\n",
    "'עצמנו',\n",
    "'עצמהם',\n",
    "'עצמהן',\n",
    "'מי',\n",
    "'מה',\n",
    "'איפה',\n",
    "'היכן',\n",
    "'במקום שבו',\n",
    "'אם',\n",
    "'לאן',\n",
    "'למקום שבו',\n",
    "'מקום בו',\n",
    "'איזה',\n",
    "'מהיכן',\n",
    "'איך',\n",
    "'כיצד',\n",
    "'באיזו מידה',\n",
    "'מתי',\n",
    "'בשעה ש',\n",
    "'כאשר',\n",
    "'כש',\n",
    "'למרות',\n",
    "'לפני',\n",
    "'אחרי',\n",
    "'מאיזו סיבה',\n",
    "'הסיבה שבגללה',\n",
    "'למה',\n",
    "'מדוע',\n",
    "'לאיזו תכלית',\n",
    "'כי',\n",
    "'יש',\n",
    "'אין',\n",
    "'אך',\n",
    "'מנין',\n",
    "'מאין',\n",
    "'מאיפה',\n",
    "'יכל',\n",
    "'יכלה',\n",
    "'יכלו',\n",
    "'יכול',\n",
    "'יכולה',\n",
    "'יכולים',\n",
    "'יכולות',\n",
    "'יוכלו',\n",
    "'יוכל',\n",
    "'מסוגל',\n",
    "'לא',\n",
    "'רק',\n",
    "'אולי',\n",
    "'אין',\n",
    "'לאו',\n",
    "'אי',\n",
    "'כלל',\n",
    "'נגד',\n",
    "'אם',\n",
    "'עם',\n",
    "'אל',\n",
    "'אלה',\n",
    "'אלו',\n",
    "'אף',\n",
    "'על',\n",
    "'מעל',\n",
    "'מתחת',\n",
    "'מצד',\n",
    "'בשביל',\n",
    "'לבין',\n",
    "'באמצע',\n",
    "'בתוך',\n",
    "'דרך',\n",
    "'מבעד',\n",
    "'באמצעות',\n",
    "'למעלה',\n",
    "'למטה',\n",
    "'מחוץ',\n",
    "'מן',\n",
    "'לעבר',\n",
    "'מכאן',\n",
    "'כאן',\n",
    "'הנה',\n",
    "'הרי',\n",
    "'פה',\n",
    "'שם',\n",
    "'אך',\n",
    "'ברם',\n",
    "'שוב',\n",
    "'אבל',\n",
    "'מבלי',\n",
    "'בלי',\n",
    "'מלבד',\n",
    "'רק',\n",
    "'בגלל',\n",
    "'מכיוון',\n",
    "'עד',\n",
    "'אשר',\n",
    "'ואילו',\n",
    "'למרות',\n",
    "'אס',\n",
    "'כמו',\n",
    "'כפי',\n",
    "'אז',\n",
    "'אחרי',\n",
    "'כן',\n",
    "'לכן',\n",
    "'לפיכך',\n",
    "'מאד',\n",
    "'עז',\n",
    "'מעט',\n",
    "'מעטים',\n",
    "'במידה',\n",
    "'שוב',\n",
    "'יותר',\n",
    "'מדי',\n",
    "'גם',\n",
    "'כן',\n",
    "'נו',\n",
    "'אחר',\n",
    "'אחרת',\n",
    "'אחרים',\n",
    "'אחרות',\n",
    "'אשר',\n",
    "'או'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d864573c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77910a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('hebrewStopWords.json', 'w') as f:\n",
    "    json.dump(hebrew_stop_words, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce6f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b7e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =[]\n",
    "with open('hebrewStopWords.json', 'r') as f:\n",
    "    stop_words = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f99ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49c1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the courpus\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "train_df = pd.DataFrame()\n",
    "train_df[\"text\"] = x_token_train;\n",
    "\n",
    "train_df[\"emoji\"] = train_df.text.map(lambda x: set(extract_emojis(x)))\n",
    "\n",
    "train_df[\"clean_text\"] = train_df.text.map(lambda x: remove_URL(x))\n",
    "train_df[\"clean_text\"] = train_df.text.map(lambda x: remove_html(x))\n",
    "train_df[\"clean_text\"] = train_df.text.map(lambda x: remove_emoji(x))\n",
    "train_df[\"clean_text\"] = train_df.text.map(lambda x: remove_punct(x))\n",
    "\n",
    "train_df[\"clean_tokens\"] = train_df.clean_text.map(tokenizer.tokenize)\n",
    "\n",
    "train_df[\"target\"] = y_token_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3b3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_hsw = get_display(\" \".join(stop_words)).split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f18093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_words_wc = WordCloud(\n",
    "    font_path='Fonts/GveretLevinAlefAlefAlef-Regular.ttf',\n",
    "    background_color=\"white\",\n",
    "    stopwords=bd_hsw,\n",
    "    max_words=30,\n",
    "    max_font_size=80).generate(get_display(all_clean_words))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(all_words_wc, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c70ff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Divide the corpus to negative and positive\n",
    "\n",
    "positive_df = train_df[train_df[\"target\"] == 0]\n",
    "negative_df = train_df[train_df[\"target\"] == 1]\n",
    "\n",
    "all_positive_wc = WordCloud(\n",
    "    font_path='Fonts/GveretLevinAlefAlefAlef-Regular.ttf',\n",
    "    background_color=\"white\",\n",
    "    stopwords=bd_hsw,\n",
    "    max_words=30,\n",
    "    max_font_size=80).generate(get_display(\" \".join(positive_df.clean_text)))\n",
    "\n",
    "all_negative_wc = WordCloud(\n",
    "    font_path='Fonts/GveretLevinAlefAlefAlef-Regular.ttf',\n",
    "    background_color=\"white\",\n",
    "    stopwords=bd_hsw,\n",
    "    max_words=30,\n",
    "    max_font_size=80).generate(get_display(\" \".join(negative_df.clean_text)))\n",
    "\n",
    "plt.figure(1, figsize=(16, 8))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"positive\")\n",
    "plt.imshow(all_positive_wc)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"negative\")\n",
    "plt.imshow(all_negative_wc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30801bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e045b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def count_vect(data, ngrams=(1, 1)):\n",
    "    count_vectorizer = CountVectorizer(ngram_range=ngrams)\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aefcce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts, count_vectorizer = count_vect(train_df[\"clean_text\"])\n",
    "test_counts = count_vectorizer.transform(x_token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ca2ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd1f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts.todense().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b72b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa0adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6c2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8368c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e989e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_counts.todense()[0][0:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bca5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_counts, count_vectorizer = count_vect(train_df.clean_text, ngrams=(1, 2))\n",
    "test_counts = count_vectorizer.transform(x_token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e13a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_counts.todense()[0][0:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b74149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def tfidf(data, ngrams=(1, 1)):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngrams)\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "\n",
    "train_tfidf, tfidf_vectorizer = tfidf(train_df.clean_text)\n",
    "test_tfidf = tfidf_vectorizer.transform(x_token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd554cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in train_tfidf.todense()[0][0:].tolist()[0] if x != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454dc3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tfidf.todense()[0][0:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199542ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_tfidf, tfidf_vectorizer = tfidf(train_df.clean_text, ngrams=(1, 2))\n",
    "test_tfidf = tfidf_vectorizer.transform(x_token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99393d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tfidf.todense()[0][0:].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fe0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00759844",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_counts = train_counts\n",
    "X_tfidf = train_tfidf\n",
    "y = y_token_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9915654",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb01cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bae44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame()\n",
    "test_df[\"text\"] = x_token_test;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6d7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8253bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def tfidf(data, ngrams=(1, 1)):\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngrams)\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "\n",
    "train_tfidf, tfidf_vectorizer = tfidf(train_df.clean_text)\n",
    "test_tfidf = tfidf_vectorizer.transform(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1123a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_df.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95879c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf, tfidf_vectorizer = tfidf(train_df[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_tfidf\n",
    "y = y_token_train\n",
    "\n",
    "X_train_tfidf, X_test, y_train_tfidf, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae42dff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4997bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight=\"balanced\")\n",
    "model.fit(X_train_tfidf, y_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcad5af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c9353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50f6f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb166a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(1 - abs((y_test - y_pred).sum() / len(y_pred))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3710954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4184c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1score = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"Tfidf Model Score: {f1score * 100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c8413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22763751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65125e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfidf = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
